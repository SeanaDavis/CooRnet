devtools::use_package()
library(devtools)
devtools::use_package()
devtools::document()
library(CooRnet)
library(CooRnet)
library(CooRnet)
library(mediacloudr)
stories3 <- get_story_list(rows = 1000,
fq = "(text:coronavirus OR text:'covid-19' OR text:'SARS-CoV-2') AND (tags_id_media:34412372 OR tags_id_media:38380117 OR tags_id_media:162546803 OR tags_id_media:162546808 OR tags_id_media:162546809) AND publish_date:[2020-03-04T00:00:00.000Z TO 2020-03-05T00:00:00.000Z]")
ctshare_output <- get_ctshares(stories3, url_column = "url", date_column = "date_published", platforms = "facebook,instagram", nmax = 500, sleep_time = 0.5, clean_urls = TRUE)
ctshare_output <- get_ctshares(stories3, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", nmax = 500, sleep_time = 0.5, clean_urls = TRUE)
#initializa log file in current directory
write(paste("#################### CooRnet #####################",
"\n", Sys.time(),
file="log.txt"))
#initializa log file in current directory
write(paste("#################### CooRnet #####################",
"\n", Sys.time()),
file="log.txt")
#initializa log file in current directory
write(paste("#################### CooRnet #####################\n",
Sys.time()),
file="log.txt")
#initializa log file in current directory
write(paste0("#################### CooRnet #####################\n",
Sys.time()),
file="log.txt")
#initializa log file in current directory
write(paste0("#################### CooRnet #####################\n",
"Script executed on ", Sys.time()),
file="log.txt")
format(Sys.time(), format = "%F %R %Z")
#initializa log file in current directory
write(paste0("#################### CooRnet #####################\n",
"Script executed on ", format(Sys.time(), format = "%F %R %Z")),
file="log.txt")
df <- get_story_list(rows = 100,
fq = "(text:coronavirus OR text:'covid-19' OR text:'SARS-CoV-2') AND publish_date:[2020-03-02T00:00:00.000Z TO 2020-04-03T00:00:00.000Z]")
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 1, nmax = 100, clean_urls = TRUE)
library(CooRnet)
library(mediacloudr)
df <- get_story_list(rows = 100,
fq = "(text:coronavirus OR text:'covid-19' OR text:'SARS-CoV-2') AND publish_date:[2020-03-02T00:00:00.000Z TO 2020-04-03T00:00:00.000Z]")
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 1, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 1, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
#initializa log file in current directory
write(paste0("#################### CooRnet #####################\n",
"Script executed on: ", format(Sys.time(), format = "%F %R %Z")),
file="log.txt")
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 1, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
#initializa log file in current directory
write(paste0("#################### CooRnet ##################### \nScript executed on: ",
format(Sys.time(), format = "%F %R %Z")),
file="log.txt")
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 1, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 1, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
ct_shares.list <- ct_shares.df
ct_shares.df <- ct_shares.list[[1]]
# unnest expanded url and clean-up
ct_shares.df <- unnest(ct_shares.df, cols = expandedLinks)
ct_shares.df$original <- NULL
if(ct_shares.df$expanded %in% df$url)
)
ct_shares.df$expanded %in% df$url
head(ct_shares.df$expanded)
head(df$url)
as.character(ct_shares.df$expanded) %in% as.character(df$url)
View(df)
df <- get_story_list(rows = 500,
fq = "(text:coronavirus OR text:'covid-19' OR text:'SARS-CoV-2') AND publish_date:[2020-03-02T00:00:00.000Z TO 2020-04-03T00:00:00.000Z]")
table(ct_shares.df$account.verified)
table(ct_shares.df$account.verified)["TRUE"]
as.numeric(table(ct_shares.df$account.verified)["TRUE"])
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
df <- get_story_list(rows = 500,
fq = "(text:coronavirus OR text:'covid-19' OR text:'SARS-CoV-2') AND publish_date:[2020-04-02T00:00:00.000Z TO 2020-04-03T00:00:00.000Z]")
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
df <- get_story_list(rows = 500,
fq = "(text:coronavirus OR text:'covid-19' OR text:'SARS-CoV-2') AND publish_date:[2020-03-02T00:00:00.000Z TO 2020-03-03T00:00:00.000Z]")
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
library(CooRnet)
df <- get_story_list(rows = 500,
fq = "(text:coronavirus OR text:'covid-19' OR text:'SARS-CoV-2') AND publish_date:[2020-04-01T00:00:00.000Z TO 2020-04-03T00:00:00.000Z]")
library(CooRnet)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, orig_urls_only = TRUE)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, orig_urls_only = FALSE)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, orig_urls_only = TRUE)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, orig_urls_only = FALSE)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, orig_urls_only = FALSE)
library(CooRnet)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, orig_urls_only = FALSE)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, orig_urls_only = FALSE)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, orig_urls_only = FALSE)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, orig_urls_only = FALSE)
library(CooRnet)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, orig_urls_only = FALSE)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, orig_urls_only = TRUE)
git checkout -- ./R
rm(ct_shares.list)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
# initialize logfile
write(paste("#################### CooRnet #####################",
"\nScript executed on: ", format(t, format = "%F %R %Z")),
file="log.txt")
# initialize logfile
write(paste("#################### CooRnet #####################",
"\nScript executed on: ", format(t, format = "%F %R %Z")),
file="log.txt")
# initialize logfile
write(paste("#################### CooRnet #####################",
"\nScript executed on: ", format(Sys.time(), format = "%F %R %Z")),
file="log.txt")
# write log
write(paste("\nOriginal URLs:", nrow(urls),
"\nCT shares:", nrow(ct_shares.df),
"\nUnique URLs in CT shares:", length(unique(ct_shares.df$expanded)),
"\nLink in CT shares matching original URLs:", as.numeric(table(ct_shares.df$account.verified)["TRUE"])),
file="log.txt",
append = TRUE)
# write log
write(paste(#"\nOriginal URLs:", nrow(urls),
"\nCT shares:", nrow(ct_shares.df),
"\nUnique URLs in CT shares:", length(unique(ct_shares.df$expanded)),
"\nLink in CT shares matching original URLs:", as.numeric(table(ct_shares.df$account.verified)["TRUE"])),
file="log.txt",
append = TRUE)
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
# write log
write(paste(#"\nOriginal URLs:", nrow(urls),
"\nCT shares:", nrow(ct_shares.df),
"\nUnique URLs in CT shares:", length(unique(ct_shares.df$expanded)),
"\nLink in CT shares matching original URLs:", as.numeric(table(ct_shares.df$account.verified)["TRUE"])), file = "log.txt", append = TRUE)
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
# remove shares performed more than one week from first share
ct_shares.df <- ct_shares.df %>%
group_by(expanded) %>%
filter(difftime(max(date), min(date), units = "secs") <= 604800)
# remove duplicates created by the unnesting
ct_shares.df <- ct_shares.df[!duplicated(ct_shares.df[,c("id", "platformId", "postUrl", "expanded")]),]
rm(output)
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
library(CooRnet)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
head(ct_shares.df$expanded, 15)
head(df$url, 15)
df <- get_story_list(rows = 500,
fq = "(text:coronavirus OR text:covid-19) AND (tags_id_media:34412372 OR tags_id_media:38380117 OR tags_id_media:162546803 OR tags_id_media:162546808 OR tags_id_media:162546809) AND publish_date:[2020-03-04T00:00:00.000Z TO 2020-03-05T00:00:00.000Z]")
View(df)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
library(CooRnet)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, orig_urls_only = TRUE)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, keep_ourl_only = TRUE)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, keep_ourl_only = FALSE)
library(CooRnet)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, keep_ourl_only = FALSE)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, keep_ourl_only = TRUE)
table(ct_shares.df$is_orig)
ct_shares.df <- subset(ct_shares.df, ct_shares.df$is_orig==TRUE)
ct_shares.df <- get_ctshares(df, url_column = "url", date_column = "publish_date", platforms = "facebook,instagram", sleep_time = 0, nmax = 100, clean_urls = TRUE, save_ctapi_output = TRUE)
library(CooRnet)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, keep_ourl_only = TRUE)
rlang::last_error()
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE)
library(CooRnet)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, keep_ourl_only = TRUE)
estimate_coord_interval(ct_shares.df = ct_shares.df, clean_urls = TRUE, keep_ourl_only = TRUE)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, orig_urls_only = TRUE)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, keep_ourl_only = TRUE)
library(CooRnet)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, keep_ourl_only = TRUE)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, keep_ourl_only = FALSE)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, keep_ourl_only = FALSE)
library(CooRnet)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, keep_ourl_only = FALSE)
library(CooRnet)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, keep_ourl_only = FALSE)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, keep_ourl_only = TRUE)
library(CooRnet)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, keep_ourl_only = TRUE)
library(CooRnet)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, keep_ourl_only = FALSE)
library(CooRnet)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, keep_ourl_only = FALSE)
output <- get_coord_shares(ct_shares.df = ct_shares.df, parallel = TRUE, clean_urls = TRUE, keep_ourl_only = TRUE)
library(CooRnet)
urls <- CooRnet::get_urls_from_ct_histdata("https://ct-download.s3.us-west-2.amazonaws.com/2020-04-04-07-08-36-EDT-Historical-Report-AMLONOESTSSOLO-1970-01-01--2020-04-04.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200404T110838Z&X-Amz-SignedHeaders=host&X-Amz-Expires=604799&X-Amz-Credential=AKIAV4ALADSXQNHZB3MA%2F20200404%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=636c0f12fbb9da956742cdf050ac0917aba722ae5f917697fbf0bbb76aec8ef7")
urls <- CooRnet::get_urls_from_ct_histdata("https://ct-download.s3.us-west-2.amazonaws.com/2020-04-04-07-08-36-EDT-Historical-Report-AMLONOESTSSOLO-1970-01-01--2020-04-04.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200404T110838Z&X-Amz-SignedHeaders=host&X-Amz-Expires=604799&X-Amz-Credential=AKIAV4ALADSXQNHZB3MA%2F20200404%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=636c0f12fbb9da956742cdf050ac0917aba722ae5f917697fbf0bbb76aec8ef7")
library(CooRnet)
urls <- CooRnet::get_urls_from_ct_histdata("https://ct-download.s3.us-west-2.amazonaws.com/2020-04-04-07-08-36-EDT-Historical-Report-AMLONOESTSSOLO-1970-01-01--2020-04-04.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200404T110838Z&X-Amz-SignedHeaders=host&X-Amz-Expires=604799&X-Amz-Credential=AKIAV4ALADSXQNHZB3MA%2F20200404%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=636c0f12fbb9da956742cdf050ac0917aba722ae5f917697fbf0bbb76aec8ef7")
library(dplyr)
urls <- CooRnet::get_urls_from_ct_histdata("https://ct-download.s3.us-west-2.amazonaws.com/2020-04-04-07-08-36-EDT-Historical-Report-AMLONOESTSSOLO-1970-01-01--2020-04-04.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200404T110838Z&X-Amz-SignedHeaders=host&X-Amz-Expires=604799&X-Amz-Credential=AKIAV4ALADSXQNHZB3MA%2F20200404%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=636c0f12fbb9da956742cdf050ac0917aba722ae5f917697fbf0bbb76aec8ef7")
library(CooRnet)
library(CooRnet)
library(CooRnet)
library(CooRnet)
library(CooRnet)
ct_shares.df <- CooRnet::get_ctshares(urls[1:100])
library(CooRnet)
df <- urls[1:100]
df <- urls[,1:100]
df <- urls[1:100,]
ct_shares.df <- CooRnet::get_ctshares(df)
ct_shares.df <- CooRnet::get_ctshares(df, url_column = "url, url_column = "date")
ct_shares.df <- CooRnet::get_ctshares(df, url_column = "url", url_column = "date")
ct_shares.df <- CooRnet::get_ctshares(df, url_column = "url", date_column = "date")
library(readr)
g8 <- read_csv("~/LabWebMKTG/rawdata/g8.csv")
urls <- g8
rn(g8)
rm(g8)
require(dplyr)     # 0.8.3
# keep only useful columns and remove duplicates
urls <- urls %>%
select(url=url_column, date=date_column) %>%
distinct
names(urls)
url_column <- "url"
date_column <- "publish_date"
# keep only useful columns and remove duplicates
urls <- urls %>%
select(url=url_column, date=date_column) %>%
distinct
names(uurls
)
names(urls)
library(readr)
urls <- read_csv("~/LabWebMKTG/rawdata/g8.csv")
# keep only useful columns and remove duplicates
urls <- urls %>%
select(url=all_of(url_column), date=all_of(date_column)) %>%
distinct
library(readr)
urls <- read_csv("~/LabWebMKTG/rawdata/g8.csv")
# keep only useful columns and remove duplicates
urls <- urls %>%
select(url=all_of(url_column), date=all_of(date_column))
View(urls)
library(readr)
urls <- read_csv("~/LabWebMKTG/rawdata/g8.csv")
# keep only useful columns and remove duplicates
urls <- urls %>%
select(url=all_of(url_column), date=all_of(date_column)) %>%
group_by(url()) %>%
summarize(date = min(date))
# keep only useful columns and remove duplicates
urls <- urls %>%
select(url=all_of(url_column), date=all_of(date_column)) %>%
group_by(url) %>%
summarize(date = min(date))
library(readr)
urls <- read_csv("~/LabWebMKTG/rawdata/g8.csv")
# keep only useful columns and remove duplicates
urls <- urls %>%
select(url=all_of(url_column), date_column) %>%
group_by(url) %>%
summarize(date = min(date_column))
library(readr)
urls <- read_csv("~/LabWebMKTG/rawdata/g8.csv")
select(url=all_of(url_column), date_column)
select(url_column, date_column)
select(url_column, date_column)
require(dplyr)     # 0.8.3
select(url_column, date_column)
select(url, url_column, date_column)
select(urls, url_column, date_column)
urls1 <- select(urls, url_column, date_column)
urls1 <- select(urls, url=url_column, date=date_column)
names(urls1)
dplyr::distinct(urls1)
urls2 <- dplyr::distinct(urls1)
urls2 <- unique(urls1)
urls2 <- group_by(urls1, url)
head(urls2)
urls3 <- summarise(urls2, date = min(date))
urls4 <- urls3[!duplicated(urls3),]
View(urls3)
durls <- urls3$url[duplicated(urls3$urls),]
durls <- urls3[duplicated(urls3$urls),]
durls <- urls3[duplicated(urls),]
durls <- urls3[duplicated(urls3),]
rm(urls*)
# keep only useful columns and remove duplicates
urls <- urls %>%
select(url=url_column, date=date_column)
# keep only useful columns and remove duplicates
urls <- urls %>%
select("url"=url_column, "date"=date_column)
